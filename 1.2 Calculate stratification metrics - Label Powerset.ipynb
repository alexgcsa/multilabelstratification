{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "import time, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pylocker import Locker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_from_arff, load_dataset_dump\n",
    "import cPickle as pickle\n",
    "import copy\n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sets = {\n",
    "    'bibtex': 159,\n",
    "    'Corel5k': 374,\n",
    "    'delicious': 983,\n",
    "    'genbase': 27,\n",
    "    'emotions': 6,\n",
    "    'enron': 53,\n",
    "    'mediamill': 101,\n",
    "    'medical': 45,\n",
    "    'scene': 6,\n",
    "    'tmc2007-500': 22,\n",
    "    'yeast': 14,\n",
    "    'rcv1subset1': 101,\n",
    "    'rcv1subset2': 101,\n",
    "    'rcv1subset3': 101,\n",
    "    'rcv1subset4': 101,\n",
    "    'rcv1subset5': 101,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initialize the experiment\n",
    "is_done = {s : [v , False] for s,v  in sets.iteritems()}\n",
    "with open(\"./prediction_lp.json\", \"w\") as fp:\n",
    "    fp.write(jsonpickle.dumps(is_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_me_set():\n",
    "    #  create a unique lock pass. This can be any string.\n",
    "    lpass = str(uuid.uuid1())\n",
    "\n",
    "    # create locker instance\n",
    "    #CHANGE\n",
    "    FL = Locker(filePath=\"./prediction_lp.json\", lockPass=lpass,mode='r+')\n",
    "\n",
    "    # acquire the lock\n",
    "    with FL as r:\n",
    "        acquired, code, fd  = r\n",
    "\n",
    "        # check if aquired.\n",
    "        if fd is not None:\n",
    "            a = jsonpickle.loads(fd.read())\n",
    "            s = filter(lambda z: a[z][1] is not True, sorted(a.keys(), key=lambda x: a[x][0]))\n",
    "            if len(s) == 0:\n",
    "                return None\n",
    "            \n",
    "            s=s[0]\n",
    "            a[s][1]=True\n",
    "            fd.seek(0)\n",
    "            fd.write(jsonpickle.dumps(a))\n",
    "            fd.truncate()\n",
    "            return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skmultilearn.problem_transform import LabelPowerset, BinaryRelevance\n",
    "from skmultilearn.cluster import IGraphLabelCooccurenceClusterer\n",
    "from skmultilearn.ensemble import LabelSpacePartitioningClassifier\n",
    "\n",
    "\n",
    "\n",
    "# partition the label space using fastgreedy community detection\n",
    "# on a weighted label co-occurrence graph with self-loops allowed\n",
    "#clusterer = IGraphLabelCooccurenceClusterer('fastgreedy', weighted=True,\n",
    "#    include_self_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_set(s):\n",
    "    data = load_dataset_dump('./dumps/{}.scikitml.bz2'.format(s))    \n",
    "\n",
    "    with open(\"./folds/{}.pickle\".format(s),\"r\") as fp:\n",
    "        fold_data = pickle.load(fp)\n",
    "\n",
    "    return data, fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def classify(s):\n",
    "    n_splits = 10\n",
    "    print s, n_splits, time.time()\n",
    "    data, fold_data = load_set(s)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "\n",
    "    label_count = y.shape[1]\n",
    "    predictions = {n : [None for i in range(n_splits)] for n in fold_data}\n",
    "    probs = {n : [None for i in range(n_splits)] for n in fold_data}\n",
    "    times = {name: [] for name in fold_data}\n",
    "    left = len(fold_data) * n_splits\n",
    "    for name, f in fold_data.iteritems():\n",
    "        for split in range(n_splits):\n",
    "            mean = np.mean([np.mean(x) if len(x) > 0 else 0.0 for x in times.values()])\n",
    "\n",
    "            t = time.time()\n",
    "            print s, name, split, str(datetime.datetime.fromtimestamp(t+mean)), str(datetime.datetime.fromtimestamp(t+left*mean))\n",
    "            left -= 1\n",
    "            \n",
    "            if len(f[split])==2:\n",
    "                train_idx = f[split][0]\n",
    "                test_idx = f[split][1]\n",
    "            else:\n",
    "                train_idx = list(chain.from_iterable([f[i] for i in xrange(n_splits) if i!=split]))\n",
    "                test_idx=f[split]\n",
    "                \n",
    "            # assifier = LabelSpacePartitioningClassifier(problem_transform_classifier, clusterer)\n",
    "            # construct base forest classifier\n",
    "            base_classifier = RandomForestClassifier(n_jobs=10)\n",
    "\n",
    "            # setup problem transformation approach with sparse matrices for random forest\n",
    "            classifier = LabelPowerset(classifier=base_classifier,\n",
    "                require_dense=[False, True])\n",
    "            classifier.fit(X[train_idx,:], y[train_idx,:])\n",
    "\n",
    "            predictions[name][split]= classifier.predict(X[test_idx,:])\n",
    "            probs[name][split]= classifier.predict_proba(X[test_idx,:])\n",
    "            t_end = time.time() - t\n",
    "            times[name].append(t_end)\n",
    "\n",
    "    with open (\"./predictions/lp/{}.pickle\".format(s), \"w\") as fp:\n",
    "        pickle.dump([predictions, probs, times], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s = get_me_set()\n",
    "while s is not None:\n",
    "    classify(s)\n",
    "    s = get_me_set()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
