{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "import time, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pylocker import Locker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_from_arff, load_dataset_dump\n",
    "import cPickle as pickle\n",
    "import copy\n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "from skmultilearn.cluster.base import LabelCooccurenceClustererBase\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "class IGraphLabelCooccurenceClusterer(LabelCooccurenceClustererBase):\n",
    "\n",
    "    \"\"\"Clusters the label space using igraph community detection methods\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    method : enum from `IGraphLabelCooccurenceClusterer.METHODS`\n",
    "        the igraph community detection method that will be used\n",
    "\n",
    "    weighted: boolean\n",
    "            Decide whether to generate a weighted or unweighted graph.\n",
    "\n",
    "    include_self_edges : boolean\n",
    "            Decide whether to include self-edge i.e. label 1 - label 1 in co-occurrence graph\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    METHODS = {\n",
    "        'fastgreedy': lambda graph, w = None: graph.community_fastgreedy(weights=w).as_clustering(),\n",
    "        'infomap': lambda graph, w = None: graph.community_infomap(edge_weights=w, trials=1000),\n",
    "        'label_propagation': lambda graph, w = None: graph.community_label_propagation(weights=w),\n",
    "        'walktrap': lambda graph, w = None: graph.community_walktrap(weights=w).as_clustering(),\n",
    "    }\n",
    "\n",
    "    def __init__(self, method=None, weighted=None, include_self_edges=None):\n",
    "        super(IGraphLabelCooccurenceClusterer, self).__init__(\n",
    "            weighted=weighted, include_self_edges=include_self_edges)\n",
    "        self.method = method\n",
    "\n",
    "        if method not in IGraphLabelCooccurenceClusterer.METHODS:\n",
    "            raise ValueError(\n",
    "                \"{} not a supported igraph community detection method\".format(method))\n",
    "\n",
    "    def fit_predict(self, X, y):\n",
    "        \"\"\"Performs clustering on y and returns list of label lists\n",
    "\n",
    "        Builds a label coocurence_graph using :func:`LabelCooccurenceClustererBase.generate_coocurence_adjacency_matrix` on `y` and then detects communities using a selected `method`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : sparse matrix (n_samples, n_features), feature space, not used in this clusterer\n",
    "        y : sparse matrix (n_samples, n_labels), label space\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        partition: list of lists : list of lists label indexes, each sublist represents labels that are in that community\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.generate_coocurence_adjacency_matrix(y)\n",
    "\n",
    "        if self.is_weighted:\n",
    "            self.weights = dict(weight=list(self.edge_map.values()))\n",
    "        else:\n",
    "            self.weights = dict(weight=None)\n",
    "\n",
    "        self.coocurence_graph = ig.Graph(\n",
    "            edges=[x for x in self.edge_map],\n",
    "            vertex_attrs=dict(name=list(range(1, self.label_count + 1))),\n",
    "            edge_attrs=self.weights\n",
    "        )\n",
    "\n",
    "        self.partition = IGraphLabelCooccurenceClusterer.METHODS[\n",
    "            self.method](self.coocurence_graph, self.weights['weight'])\n",
    "        return np.array(self.partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from builtins import zip\n",
    "from builtins import range\n",
    "from skmultilearn.problem_transform.br import BinaryRelevance\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "from skmultilearn.utils import get_matrix_in_format\n",
    "\n",
    "\n",
    "class LabelSpacePartitioningClassifier(BinaryRelevance):\n",
    "    \"\"\"Community detection base classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    classifier : scikit classifier type\n",
    "        The base classifier that will be used in a class, will be automagically put under self.classifier for future access.\n",
    "\n",
    "    clusterer: an skmultilearn.cluster.base object that partitions the output space\n",
    "\n",
    "    require_dense : [boolean, boolean]\n",
    "        Whether the base classifier requires input as dense arrays, False by default for \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classifier=None, clusterer=None, require_dense=None):\n",
    "        super(LabelSpacePartitioningClassifier, self).__init__(\n",
    "            classifier, require_dense)\n",
    "        self.clusterer = clusterer\n",
    "        self.copyable_attrs = ['clusterer', 'classifier', 'require_dense']\n",
    "\n",
    "    def generate_partition(self, X, y):\n",
    "        self.partition = self.clusterer.fit_predict(X, y)\n",
    "        self.model_count = len(self.partition)\n",
    "        self.label_count = y.shape[1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict labels for X, see base method's documentation.\"\"\"\n",
    "        X = self.ensure_input_format(\n",
    "            X, sparse_format='csr', enforce_sparse=True)\n",
    "        result = sparse.lil_matrix((X.shape[0], self.label_count), dtype=int)\n",
    "\n",
    "        for model in range(self.model_count):\n",
    "            predictions = self.ensure_output_format(self.classifiers[model].predict(\n",
    "                X), sparse_format=None, enforce_sparse=True).nonzero()\n",
    "            for row, column in zip(predictions[0], predictions[1]):\n",
    "                result[row, self.partition[model][column]] = 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict labels for X, see base method's documentation.\"\"\"\n",
    "        X = self.ensure_input_format(\n",
    "            X, sparse_format='csr', enforce_sparse=True)\n",
    "        result = sparse.lil_matrix((X.shape[0], self.label_count), dtype=float)\n",
    "\n",
    "        for model in range(self.model_count):\n",
    "            predictions = self.ensure_output_format(self.classifiers[model].predict_proba(\n",
    "                X), sparse_format=None, enforce_sparse=True)\n",
    "            for column, label in enumerate(self.partition[model]):\n",
    "                result[:, label] = predictions[:, column]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sets = {\n",
    "    'bibtex': 159,\n",
    "    'Corel5k': 374,\n",
    "    'delicious': 983,\n",
    "    'genbase': 27,\n",
    "    'emotions': 6,\n",
    "    'enron': 53,\n",
    "    'mediamill': 101,\n",
    "    'medical': 45,\n",
    "    'scene': 6,\n",
    "    'tmc2007-500': 22,\n",
    "    'yeast': 14,\n",
    "    'rcv1subset1': 101,\n",
    "    'rcv1subset2': 101,\n",
    "    'rcv1subset3': 101,\n",
    "    'rcv1subset4': 101,\n",
    "    'rcv1subset5': 101,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initialize the experiment\n",
    "is_done = {s : [v , False] for s,v  in sets.iteritems()}\n",
    "with open(\"./prediction_graphs.json\", \"w\") as fp:\n",
    "    fp.write(jsonpickle.dumps(is_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_me_set():\n",
    "    #  create a unique lock pass. This can be any string.\n",
    "    lpass = str(uuid.uuid1())\n",
    "\n",
    "    # create locker instance\n",
    "    FL = Locker(filePath=\"./prediction_graphs.json\", lockPass=lpass,mode='r+')\n",
    "\n",
    "    # acquire the lock\n",
    "    with FL as r:\n",
    "        acquired, code, fd  = r\n",
    "\n",
    "        # check if aquired.\n",
    "        if fd is not None:\n",
    "            a = jsonpickle.loads(fd.read())\n",
    "            s = filter(lambda z: a[z][1] is not True, sorted(a.keys(), key=lambda x: a[x][0]))\n",
    "            if len(s) == 0:\n",
    "                return None\n",
    "            \n",
    "            s=s[0]\n",
    "            a[s][1]=True\n",
    "            fd.seek(0)\n",
    "            fd.write(jsonpickle.dumps(a))\n",
    "            fd.truncate()\n",
    "            return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_set(s):\n",
    "    data = load_dataset_dump('./dumps/{}.scikitml.bz2'.format(s))    \n",
    "\n",
    "    with open(\"./folds/{}.pickle\".format(s),\"r\") as fp:\n",
    "        fold_data = pickle.load(fp)\n",
    "\n",
    "    return data, fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "param_list_for_cluster = []\n",
    "for method in ['fastgreedy']:\n",
    "    for is_weighted in [True, False]:\n",
    "        param_list_for_cluster.append((method, is_weighted))\n",
    "param_list_for_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def classify(s, param_list_for_cluster):\n",
    "    n_splits = 10\n",
    "    print s, n_splits, time.time()\n",
    "    data, fold_data = load_set(s)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "\n",
    "    label_count = y.shape[1]\n",
    "    predictions = {p: {n : [None for i in range(n_splits)] for n in fold_data} for p in param_list_for_cluster}\n",
    "    probs = {p: {n : [None for i in range(n_splits)] for n in fold_data} for p in param_list_for_cluster}\n",
    "    times = {p: {name: [] for name in fold_data} for p in param_list_for_cluster}\n",
    "    modularities = {p: {name: [] for name in fold_data} for p in param_list_for_cluster}\n",
    "    partitions = {p: {name: [] for name in fold_data} for p in param_list_for_cluster}\n",
    "    left = len(fold_data) * n_splits*len(param_list_for_cluster)\n",
    "    for param_set in param_list_for_cluster:\n",
    "        method, is_weighted = param_set\n",
    "        for name, f in fold_data.iteritems():\n",
    "            for split in range(n_splits):\n",
    "                if len(f[split])==2:\n",
    "                    train_idx = f[split][0]\n",
    "                    test_idx = f[split][1]\n",
    "                else:\n",
    "                    train_idx = list(chain.from_iterable([f[i] for i in xrange(n_splits) if i!=split]))\n",
    "                    test_idx=f[split]\n",
    "\n",
    "                mean = np.mean([np.mean([np.mean(x) if len(x) > 0 else 0.0 for x in t.values()]) for t in times.values()])\n",
    "\n",
    "                t = time.time()\n",
    "                print s, name, split, method, is_weighted, str(datetime.datetime.fromtimestamp(t+mean)), str(datetime.datetime.fromtimestamp(t+left*mean))\n",
    "                left -= 1\n",
    "\n",
    "                clusterer = IGraphLabelCooccurenceClusterer(method, weighted=is_weighted, include_self_edges=False)\n",
    "\n",
    "                # construct base forest classifier\n",
    "                base_classifier = RandomForestClassifier(n_jobs=15)\n",
    "\n",
    "                # setup problem transformation approach with sparse matrices for random forest\n",
    "                pt_classifier = LabelPowerset(classifier=base_classifier,\n",
    "                    require_dense=[False, True])\n",
    "\n",
    "\n",
    "                # setup problem transformation approach with sparse matrices for random forest\n",
    "                classifier = LabelSpacePartitioningClassifier(pt_classifier, clusterer)\n",
    "                classifier.fit(X[train_idx,:], y[train_idx,:])\n",
    "                \n",
    "                modularities[param_set][name].append(classifier.clusterer.partition.modularity)\n",
    "                partitions[param_set][name].append(copy.copy(classifier.clusterer.partition))\n",
    "\n",
    "                predictions[param_set][name][split]= classifier.predict(X[test_idx,:])\n",
    "                probs[param_set][name][split]= classifier.predict_proba(X[test_idx,:])\n",
    "                t_end = time.time() - t\n",
    "                times[param_set][name].append(t_end)\n",
    "\n",
    "    with open (\"./predictions/graphs/{}-{}-{}.pickle\".format(s,method,is_weighted), \"w\") as fp:\n",
    "        pickle.dump([predictions, probs, times, partitions, modularities], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s = get_me_set()\n",
    "while s is not None:\n",
    "    classify(s,param_list_for_cluster)\n",
    "    s = get_me_set()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
